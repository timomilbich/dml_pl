model:
  base_learning_rate: 0.00001
  weight_decay: 0.0004
  gamma: 0.3
  tau: [1000]
  scheduler: "step"
  target: models.dml_model.DML_Model
  params:
    config:
      Architecture:
        target: architectures.bninception.Network
        params:
          pretraining: "imagenet"
          embed_dim: 512
          arch: "bninception_frozen_normalize"

      Loss:
        target: criteria.select
        name: "multisimilarity"
        params:
          name: "multisimilarity"
          batchminer:
          n_classes: 100
          loss_multisimilarity_pos_weight: 2
          loss_multisimilarity_neg_weight: 40
          loss_multisimilarity_margin: 0.1
          loss_multisimilarity_pos_thresh: 0.5
          loss_multisimilarity_neg_thresh: 0.5
          loss_multisimilarity_base_mode: 1
          loss_multisimilarity_d_mode: 'cosine'

      Batchmining:
        target: batchminer.select
        name: "distance"
        params:
          name: "distance"
          miner_distance_lower_cutoff: 0.5
          miner_distance_upper_cutoff: 1.4

      Evaluation:
        target: metrics.metric_computer.MetricComputer
        params:
          metric_names: ["e_recall@1", "nmi"]
          n_classes: 100
          evaluate_on_gpu: True
          num_workers: 0

      CustomLogs:
        target: models.log.custom_logging

data:
  target: data.base.DataModuleFromConfig
  params:
    batch_size: 112
    num_workers: 20

    train:
      target: data.CUB200.CUB200DATA
      params: 
        root: "/export/home/karoth/Datasets/cub200/"
        train: True
        arch: "bninception_frozen_normalize"
      data_sampler:
        target: datasampler.select
        params:
          name: "class_random"
          samples_per_class: 2

    validation:
      target: data.CUB200.CUB200DATA
      params:
        root: "/export/home/karoth/Datasets/cub200/"
        train: False
        arch: "bninception_frozen_normalize"

lightning:
  trainer:
    # accelerator: 'ddp'
    # accumulate_grad_batches: 1
    auto_scale_batch_size: false
    benchmark: False
    deterministic: True
    amp_backend: 'native'
    #amp_level: '02'
    log_every_n_steps: 25
    check_val_every_n_epoch: 1
    max_epochs: 100

  logger:
    target: pytorch_lightning.loggers.WandbLogger
    params:
      wandb_key: 8388187e7c47589ca2875e4007015c7536aede7f
      project: DML_PL
      group: baselines
      savename:

  modelcheckpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      dirpath: ''
      monitor: 'val/e_recall@1'
      mode: min
      period: 1
      verbose: True
      save_last: True
      save_top_k: 1
  
  callbacks:
#    earlystopcallback:
#      target: utils.callbacks.EarlyStoppingPL
#      params:
#        monitor: 'val/accuracy'
#        min_delta: 0.0001
